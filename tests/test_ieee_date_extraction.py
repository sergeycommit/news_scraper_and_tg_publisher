#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç —Å IEEE Spectrum
"""

import requests
from bs4 import BeautifulSoup
import re
from datetime import date
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from ieee_spectrum_scraper import IEEESpectrumScraper

def test_ieee_date_extraction():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞—Ç —Å IEEE Spectrum"""
    
    scraper = IEEESpectrumScraper()
    
    # URL –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    test_url = "https://spectrum.ieee.org/thunderforge-ai-wargames-dod"
    
    print(f"üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç —Å: {test_url}")
    print("=" * 60)
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        response = requests.get(test_url, headers=scraper.headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        print("üìÑ –ê–Ω–∞–ª–∏–∑ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
        
        # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å –¥–∞—Ç–∞–º–∏
        date_selectors = [
            '[class*="date"]',
            'time',
            '.date', '.time', '[data-testid="date"]',
            '.article-date', '.post-date', '.published-date',
            '.meta-date', '.timestamp', '.publish-date',
            '.byline', '.author-info', '.meta'
        ]
        
        found_dates = []
        
        for selector in date_selectors:
            elements = soup.select(selector)
            if elements:
                print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω—ã —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º '{selector}': {len(elements)}")
                
                for i, elem in enumerate(elements[:5]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                    text = elem.get_text(strip=True)
                    datetime_attr = elem.get('datetime')
                    
                    print(f"  {i+1}. –¢–µ–∫—Å—Ç: '{text}'")
                    if datetime_attr:
                        print(f"     datetime: '{datetime_attr}'")
                    
                    if text and any(char.isdigit() for char in text):
                        found_dates.append({
                            'text': text,
                            'datetime': datetime_attr,
                            'selector': selector
                        })
        
        print(f"\nüìÖ –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–∞—Ç: {len(found_dates)}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–π –Ω–∞–π–¥–µ–Ω–Ω–æ–π –¥–∞—Ç—ã
        print("\nüîß –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç:")
        print("-" * 40)
        
        for i, date_info in enumerate(found_dates):
            print(f"\n{i+1}. –¢–µ–∫—Å—Ç: '{date_info['text']}'")
            if date_info['datetime']:
                print(f"   datetime: '{date_info['datetime']}'")
            
            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
            parsed_date = scraper.parse_article_date(date_info['text'])
            if parsed_date:
                print(f"   ‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω: {parsed_date}")
                print(f"   üìÖ –°–µ–≥–æ–¥–Ω—è: {scraper.today}")
                print(f"   üéØ –°—Ç–∞—Ç—å—è –∑–∞ —Å–µ–≥–æ–¥–Ω—è: {parsed_date == scraper.today}")
            else:
                print(f"   ‚ùå –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è")
            
            # –¢–∞–∫–∂–µ –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å datetime –∞—Ç—Ä–∏–±—É—Ç
            if date_info['datetime']:
                parsed_datetime = scraper.parse_article_date(date_info['datetime'])
                if parsed_datetime:
                    print(f"   ‚úÖ datetime –ø–∞—Ä—Å–∏–Ω–≥: {parsed_datetime}")
                    print(f"   üéØ –°—Ç–∞—Ç—å—è –∑–∞ —Å–µ–≥–æ–¥–Ω—è (datetime): {parsed_datetime == scraper.today}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å—Ç–∞—Ç—å–µ
        print(f"\nüì∞ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å—Ç–∞—Ç—å–µ:")
        print("-" * 50)
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å—Ç–∞—Ç—å–∏
        article_selectors = [
            'article',
            '.article-content',
            '.post-content',
            '.entry-content',
            '.content',
            'main'
        ]
        
        article_element = None
        for selector in article_selectors:
            elements = soup.select(selector)
            if elements:
                article_element = elements[0]
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å—Ç–∞—Ç—å–∏: {selector}")
                break
        
        if article_element:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∞—Ç—å–µ
            article_info = scraper.extract_article_info(article_element, "AI")
            if article_info:
                print(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {article_info['title']}")
                print(f"‚úÖ –°—Å—ã–ª–∫–∞: {article_info['link']}")
                print(f"‚úÖ –ê–≤—Ç–æ—Ä: {article_info['author']}")
                print(f"‚úÖ –î–∞—Ç–∞ (—Ç–µ–∫—Å—Ç): {article_info['date']}")
                print(f"‚úÖ –î–∞—Ç–∞ (–ø–∞—Ä—Å–∏–Ω–≥): {article_info['parsed_date']}")
                print(f"‚úÖ –°—Ç–∞—Ç—å—è –∑–∞ —Å–µ–≥–æ–¥–Ω—è: {article_info['parsed_date'] == scraper.today if article_info['parsed_date'] else False}")
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∞—Ç—å–µ")
        else:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å—Ç–∞—Ç—å–∏")
        
        # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ AI
        print(f"\nüîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ AI:")
        print("-" * 50)
        
        ai_articles = scraper.scrape_topic_page("https://spectrum.ieee.org/topic/artificial-intelligence", "AI")
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è: {len(ai_articles)}")
        
        for i, article in enumerate(ai_articles[:3]):
            print(f"\n{i+1}. {article['title']}")
            print(f"   –î–∞—Ç–∞: {article['date']}")
            print(f"   –ü–∞—Ä—Å–∏–Ω–≥: {article['parsed_date']}")
            print(f"   –°—Å—ã–ª–∫–∞: {article['link']}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_ieee_date_extraction() 
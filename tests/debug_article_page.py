#!/usr/bin/env python3
"""
Debug script –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏ IEEE Spectrum
"""

import requests
from bs4 import BeautifulSoup
import re

def find_article_url():
    """–ü–æ–∏—Å–∫ URL —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ robotics"""
    print("üîç –ü–æ–∏—Å–∫ URL —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ robotics...")
    
    url = "https://spectrum.ieee.org/topic/robotics"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏
        articles = soup.find_all('article')
        print(f"üì∞ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
        
        if articles:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Å—Ç–∞—Ç—å—é
            first_article = articles[0]
            
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            title_elem = first_article.find('h2') or first_article.find('h3')
            if title_elem:
                title = title_elem.get_text(strip=True)
                print(f"üìù –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø–µ—Ä–≤–æ–π —Å—Ç–∞—Ç—å–∏: {title}")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç–∞—Ç—å–∏
            print(f"\nüèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç–∞—Ç—å–∏:")
            for i, child in enumerate(first_article.children):
                if hasattr(child, 'name') and child.name:
                    print(f"   {i}: <{child.name}> {child.get('class', '')}")
                    if child.name == 'a':
                        href = child.get('href')
                        text = child.get_text(strip=True)
                        print(f"      href='{href}' text='{text}'")
            
            # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ —Å—Ç–∞—Ç—å–µ
            links = first_article.find_all('a')
            print(f"\nüîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –≤ —Å—Ç–∞—Ç—å–µ: {len(links)}")
            
            for i, link in enumerate(links):
                href = link.get('href')
                text = link.get_text(strip=True)
                print(f"   –°—Å—ã–ª–∫–∞ {i+1}: {href} - '{text}'")
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é (–Ω–µ –Ω–∞ topic, –Ω–µ –Ω–∞ type)
                if href and not any(exclude in href for exclude in ['/topic/', '/type/', 'spectrum.ieee.org/topic/', 'spectrum.ieee.org/type/']):
                    if href.startswith('/'):
                        article_url = f"https://spectrum.ieee.org{href}"
                    elif href.startswith('http'):
                        article_url = href
                    else:
                        continue
                    
                    print(f"üîó –ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç–∞—Ç—å—é: {article_url}")
                    return article_url
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Å—ã–ª–∫—É, –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
            print(f"\nüîç –ü–æ–∏—Å–∫ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É...")
            if title:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞
                for link in links:
                    href = link.get('href')
                    text = link.get_text(strip=True)
                    if text and any(word in text.lower() for word in ['deepmind', 'robots', 'tennis']):
                        if href.startswith('/'):
                            article_url = f"https://spectrum.ieee.org{href}"
                        elif href.startswith('http'):
                            article_url = href
                        else:
                            continue
                        
                        print(f"üîó –ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É: {article_url}")
                        return article_url
        
        return None
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç—å–∏: {e}")
        return None

def analyze_article_page(url):
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏ IEEE Spectrum"""
    print(f"üîç –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏: {url}")
    print("=" * 60)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã picture
        pictures = soup.find_all('picture')
        print(f"üì∏ –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ picture: {len(pictures)}")
        
        for i, picture in enumerate(pictures):
            print(f"\nüîç Picture {i+1}:")
            
            # –ò—â–µ–º source —ç–ª–µ–º–µ–Ω—Ç—ã
            sources = picture.find_all('source')
            print(f"   üìπ Sources: {len(sources)}")
            
            for j, source in enumerate(sources):
                srcset = source.get('srcset')
                src = source.get('src')
                media = source.get('media')
                type_attr = source.get('type')
                
                print(f"     Source {j+1}:")
                print(f"       srcset: {srcset}")
                print(f"       src: {src}")
                print(f"       media: {media}")
                print(f"       type: {type_attr}")
            
            # –ò—â–µ–º img —ç–ª–µ–º–µ–Ω—Ç
            img = picture.find('img')
            if img:
                src = img.get('src')
                alt = img.get('alt')
                width = img.get('width')
                height = img.get('height')
                
                print(f"   üñºÔ∏è Img:")
                print(f"     src: {src}")
                print(f"     alt: {alt}")
                print(f"     width: {width}")
                print(f"     height: {height}")
        
        # –ò—â–µ–º –≤—Å–µ img —ç–ª–µ–º–µ–Ω—Ç—ã
        images = soup.find_all('img')
        print(f"\nüñºÔ∏è –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
        
        # –ò—â–µ–º GIF —Ñ–∞–π–ª—ã
        gif_images = []
        for img in images:
            src = img.get('src', '')
            if '.gif' in src.lower():
                gif_images.append(img)
        
        print(f"üé¨ –ù–∞–π–¥–µ–Ω–æ GIF –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(gif_images)}")
        
        for i, gif in enumerate(gif_images):
            src = gif.get('src')
            alt = gif.get('alt')
            print(f"   GIF {i+1}: {src}")
            print(f"     alt: {alt}")
        
        # –ò—â–µ–º –≤ meta —Ç–µ–≥–∞—Ö
        meta_images = []
        for meta in soup.find_all('meta'):
            property_attr = meta.get('property', '')
            name_attr = meta.get('name', '')
            content = meta.get('content', '')
            
            if any(prop in property_attr.lower() for prop in ['image', 'og:image', 'twitter:image']):
                meta_images.append((property_attr, content))
        
        print(f"\nüìã Meta –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {len(meta_images)}")
        for prop, content in meta_images:
            print(f"   {prop}: {content}")
        
        # –ò—â–µ–º –≤ JSON-LD
        scripts = soup.find_all('script', type='application/ld+json')
        print(f"\nüìÑ JSON-LD —Å–∫—Ä–∏–ø—Ç–æ–≤: {len(scripts)}")
        
        for i, script in enumerate(scripts):
            try:
                import json
                data = json.loads(script.string)
                if isinstance(data, dict) and 'image' in data:
                    print(f"   JSON-LD {i+1} image: {data['image']}")
            except:
                pass
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîç –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü —Å—Ç–∞—Ç–µ–π IEEE Spectrum")
    print("=" * 70)
    
    # –°–Ω–∞—á–∞–ª–∞ –Ω–∞–π–¥–µ–º URL —Å—Ç–∞—Ç—å–∏
    article_url = find_article_url()
    
    if article_url:
        print(f"\n{'='*20} –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç—å–∏ {'='*20}")
        analyze_article_page(article_url)
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ URL —Å—Ç–∞—Ç—å–∏")

if __name__ == "__main__":
    main() 